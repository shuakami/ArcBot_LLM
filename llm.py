import traceback
from storage.history import save_conversation_history, load_conversation_history
from llm_api import get_ai_response
from context_utils import build_context_within_limit
from storage.notebook import DEFAULT_ROLE_KEY
from logger import log, log_llm_context
from core.prompt_builder import build_system_prompt

def process_conversation(chat_id, user_input, chat_type="private", active_role_name=None):
    log.debug(f"LLM: å¼€å§‹ process_conversation, chat_id={chat_id}, chat_type={chat_type}, active_role_name='{active_role_name}'")
    
    try:
        # 1. ç¡®å®šæœ¬æ¬¡å¯¹è¯æ‰€å±çš„è§’è‰²
        role_key = active_role_name if active_role_name else DEFAULT_ROLE_KEY
        log.debug(f"LLM: æœ¬è½®å¯¹è¯ role_key ç¡®å®šä¸º: '{role_key}'")

        # 2. æ„å»ºç³»ç»Ÿæç¤º
        log.debug(f"LLM: å‡†å¤‡æ„å»º system_prompt, ä¼ å…¥ active_role_name='{active_role_name}'")
        system_prompt_content = build_system_prompt(chat_id, chat_type, active_role_name=active_role_name)
        system_message = {"role": "system", "content": system_prompt_content}
        log.debug(f"LLM: system_prompt æ„å»ºå®Œæˆ")

        # 3. åŠ è½½æ­¤è§’è‰²ä¸“å±çš„å†å²è®°å½•
        log.debug(f"LLM: å‡†å¤‡åŠ è½½å†å²è®°å½•, ä¼ å…¥ active_role_name='{active_role_name}'")
        history = load_conversation_history(chat_id, chat_type, active_role_name=active_role_name)
        log.debug(f"LLM: å†å²è®°å½•åŠ è½½å®Œæˆ, å…± {len(history)} æ¡")
        
        # ç¡®ä¿system promptæ˜¯æœ€æ–°çš„
        if history and history[0]['role'] == 'system':
            history[0] = system_message
            log.debug("LLM: æ›´æ–°äº†å†å²è®°å½•ä¸­çš„ system_prompt")
        else:
            history.insert(0, system_message)
            log.debug("LLM: åœ¨å†å²è®°å½•å¼€å¤´æ’å…¥äº†æ–°çš„ system_prompt")
            
        # 4. æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        user_message = {"role": "user", "content": user_input, "role_marker": role_key}
        history.append(user_message)
        log.debug(f"LLM: æ·»åŠ äº†ç”¨æˆ·æ–°æ¶ˆæ¯, å½“å‰å†å²å…± {len(history)} æ¡")

        # 5. æ„å»ºå¹¶å‘é€ä¸Šä¸‹æ–‡åˆ°AI
        context_to_send = build_context_within_limit(history, active_role=role_key)
        log.debug("LLM: å¼€å§‹æ„å»ºå‘é€åˆ°AIçš„æœ€ç»ˆä¸Šä¸‹æ–‡")
        log_llm_context(context_to_send)
        
        # 6. å¤„ç† AI å“åº”æµ
        full_response = ""
        response_segments = []
        log.debug("LLM: å¼€å§‹è°ƒç”¨AIæ¥å£...")
        for segment in get_ai_response(context_to_send):
            # æ—¥å¿—å·²åœ¨ chat_service ä¸­è®°å½•ï¼Œæ­¤å¤„ä¸å†é‡å¤
            response_segments.append(segment)
            yield segment
        
        full_response = "".join(response_segments)
        log.debug(f"LLM: AIå›å¤å®Œæˆ, æ€»é•¿åº¦: {len(full_response)}")

        # 7. ä¿å­˜å†å²è®°å½•
        if full_response:
            ai_message = {"role": "assistant", "content": full_response, "role_marker": role_key}
            history.append(ai_message)
            log.debug(f"LLM: å‡†å¤‡ä¿å­˜å†å²è®°å½•, ä¼ å…¥ active_role_name='{active_role_name}', å…± {len(history)} æ¡")
            
            save_conversation_history(
                chat_id, 
                history, 
                chat_type, 
                active_role_name=active_role_name
            )
            log.info("ğŸ’¾ å¯¹è¯å†å²å·²ä¿å­˜")

    except Exception:
        log.error("AIå“åº”å‡ºé”™", exc_info=True)
        yield "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶é‡åˆ°äº†å†…éƒ¨é”™è¯¯ã€‚"
        return
