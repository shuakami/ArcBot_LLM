import os
import sys
import requests
import zipfile
import shutil
import json
import hashlib
import urllib.parse # Ensure urllib.parse is imported at the top level
from packaging.version import parse as parse_version
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False

from logger import get_logger # Import the new logger
logger = get_logger(__name__) # Get a logger for this module

# ANSIé¢œè‰²å®šä¹‰ (can still be used with the logger)
class LogColor:
    END = '\033[0m'
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    CYAN = '\033[36m'
    BOLD = '\033[1m'

def log_info(msg):
    logger.info(f"{LogColor.BLUE}{msg}{LogColor.END}")

def log_success(msg):
    logger.info(f"{LogColor.GREEN}{msg}{LogColor.END}") # Using logger.info for success as well, color indicates success

def log_warning(msg):
    logger.warning(f"{LogColor.YELLOW}{msg}{LogColor.END}")

def log_error(msg):
    logger.error(f"{LogColor.RED}{msg}{LogColor.END}")

# API ç«¯ç‚¹å’Œé•œåƒåˆ—è¡¨
API_URL = "https://webhook.sdjz.wiki/api/latest_release_info"
VERSION_FILE = "VERSION.txt"
CONFIG_DIR = "config"

# å¤‡ç”¨ä¸‹è½½é•œåƒæ¨¡æ¿
DOWNLOAD_MIRRORS = [
    "https://github.com/{path}", # å®˜æ–¹é“¾æ¥æ”¾ç¬¬ä¸€ä¸ª
    "https://99z.top/https://github.com/{path}",
    "https://gh-proxy.com/https://github.com/{path}",
    "https://ghproxy.net/https://github.com/{path}",
    "https://github.tbedu.top/{path}",
]

def get_current_version():
    """è¯»å–æœ¬åœ°ç‰ˆæœ¬æ–‡ä»¶"""
    if not os.path.exists(VERSION_FILE):
        return parse_version("0.0.0") # é»˜è®¤ä¸º0.0.0
    with open(VERSION_FILE, 'r') as f:
        return parse_version(f.read().strip())

def get_latest_release_info():
    """
    ä»APIè·å–æœ€æ–°ç‰ˆæœ¬ä¿¡æ¯.
    æœŸæœ›APIå“åº”çš„JSONä¸­åŒ…å« 'latest_version', 'release_notes', 
    'source_code_zip_url', å’Œ 'sha256_hash' (zipæ–‡ä»¶çš„SHA256å“ˆå¸Œå€¼).
    """
    try:
        response = requests.get(API_URL, timeout=10)
        response.raise_for_status() 
        return response.json()
    except requests.exceptions.HTTPError as e:
        log_error(f"è·å–æœ€æ–°ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥ (HTTPé”™è¯¯): {e.response.status_code} - {e.response.reason}")
        return None
    except requests.exceptions.ConnectionError as e:
        log_error(f"è·å–æœ€æ–°ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥ (ç½‘ç»œè¿æ¥é”™è¯¯): {e}")
        return None
    except requests.exceptions.Timeout as e:
        log_error(f"è·å–æœ€æ–°ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥ (è¯·æ±‚è¶…æ—¶): {e}")
        return None
    except requests.exceptions.RequestException as e: # Catch other request-related errors
        log_error(f"è·å–æœ€æ–°ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥ (è¯·æ±‚å¼‚å¸¸): {e}")
        return None
    except json.JSONDecodeError as e:
        log_error(f"è§£æAPIå“åº”JSONå¤±è´¥: {e}") # More specific message
        return None
    except Exception as e: # Generic fallback
        log_error(f"è·å–æœ€æ–°ç‰ˆæœ¬ä¿¡æ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        logger.exception("Uncaught exception in get_latest_release_info", exc_info=True)
        return None


def calculate_sha256(filepath):
    """è®¡ç®—æ–‡ä»¶çš„SHA256å“ˆå¸Œå€¼"""
    sha256_hash = hashlib.sha256()
    with open(filepath, "rb") as f:
        # Read and update hash string value in blocks of 4K
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

def download_file(url_path, download_path, expected_hash=None):
    """
    å°è¯•ä»å®˜æ–¹å’Œé•œåƒåˆ—è¡¨ä¸‹è½½æ–‡ä»¶ï¼Œå¸¦è¿›åº¦æ¡ã€‚
    å¦‚æœæä¾›äº† expected_hashï¼Œåˆ™è¿›è¡ŒSHA256æ ¡éªŒã€‚
    è¿”å›ä¸‹è½½æˆåŠŸä¸å¦ (True/False)ã€‚
    """
    urls_to_try = [mirror.format(path=url_path) for mirror in DOWNLOAD_MIRRORS]
    for i, full_url in enumerate(urls_to_try):
        try:
            log_info(f"ä¸‹è½½ [{i+1}/{len(urls_to_try)}] {full_url}")
            headers = {}
            if "api.github.com" in full_url:
                headers['Accept'] = 'application/octet-stream'
            with requests.get(full_url, headers=headers, stream=True, timeout=60) as r:
                r.raise_for_status()
                total = int(r.headers.get('content-length', 0))
                if HAS_TQDM and total > 0:
                    with open(download_path, 'wb') as f, tqdm(total=total, unit='B', unit_scale=True, desc='ä¸‹è½½è¿›åº¦', ncols=70) as pbar:
                        for chunk in r.iter_content(chunk_size=8192):
                            f.write(chunk)
                            pbar.update(len(chunk))
                else:
                    downloaded = 0
                    with open(download_path, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            f.write(chunk)
                            if total > 0:
                                downloaded += len(chunk)
                                percent = int(downloaded * 100 / total)
                                # For \r progress, print might still be okay, or use logger.debug with special handling
                                sys.stdout.write(f"\rä¸‹è½½è¿›åº¦: {percent}%")
                                sys.stdout.flush()
                    if total > 0:
                        sys.stdout.write("\n") # Newline after progress
                        sys.stdout.flush()
            log_success(f"ä¸‹è½½æˆåŠŸ: {download_path}")

            if expected_hash:
                log_info(f"æ­£åœ¨æ ¡éªŒæ–‡ä»¶å“ˆå¸Œå€¼: {download_path}")
                actual_hash = calculate_sha256(download_path)
                if actual_hash.lower() == expected_hash.lower():
                    log_success("æ–‡ä»¶å“ˆå¸Œå€¼æ ¡éªŒæˆåŠŸã€‚")
                    return True
                else:
                    log_error(f"å“ˆå¸Œæ ¡éªŒå¤±è´¥ï¼æ–‡ä»¶å¯èƒ½å·²æŸåæˆ–è¢«ç¯¡æ”¹ã€‚")
                    log_error(f"  é¢„æœŸå“ˆå¸Œ: {expected_hash}")
                    log_error(f"  å®é™…å“ˆå¸Œ: {actual_hash}")
                    try:
                        os.remove(download_path)
                        log_info(f"å·²åˆ é™¤ä¸åŒ¹é…çš„ä¸‹è½½æ–‡ä»¶: {download_path}")
                    except OSError as e_del:
                        log_warning(f"åˆ é™¤ä¸åŒ¹é…æ–‡ä»¶ {download_path} å¤±è´¥: {e_del}")
                    return False # å“ˆå¸Œä¸åŒ¹é…ï¼Œå³ä½¿ä¸‹è½½æˆåŠŸä¹Ÿè¿”å›False
            else:
                # å¦‚æœæ²¡æœ‰æä¾›é¢„æœŸå“ˆå¸Œï¼Œæˆ‘ä»¬ä»…èƒ½å‡è®¾ä¸‹è½½æˆåŠŸå³ä¸ºæˆåŠŸ
                # ä½†åœ¨ check_and_update ä¸­ï¼Œå¦‚æœAPIæ²¡æœ‰æä¾›å“ˆå¸Œï¼Œä¼šä¸­æ­¢æ“ä½œ
                log_warning("æœªæä¾›é¢„æœŸå“ˆå¸Œå€¼ï¼Œè·³è¿‡æ–‡ä»¶æ ¡éªŒã€‚")
                return True # ä¸‹è½½æˆåŠŸï¼Œä½†æœªæ ¡éªŒ

        except requests.exceptions.HTTPError as e_http:
            log_error(f"ä» {full_url} ä¸‹è½½å¤±è´¥ (HTTPé”™è¯¯): {e_http.response.status_code} - {e_http.response.reason}")
        except requests.exceptions.ConnectionError as e_conn:
            log_error(f"ä» {full_url} ä¸‹è½½å¤±è´¥ (ç½‘ç»œè¿æ¥é”™è¯¯): {e_conn}")
        except requests.exceptions.Timeout as e_timeout:
            log_error(f"ä» {full_url} ä¸‹è½½å¤±è´¥ (è¯·æ±‚è¶…æ—¶): {e_timeout}")
        except requests.exceptions.RequestException as e_req: # Catch other request-related errors
            log_error(f"ä» {full_url} ä¸‹è½½å¤±è´¥ (è¯·æ±‚å¼‚å¸¸): {e_req}")
        except Exception as e: # Generic fallback for other errors during download/hash check
            log_error(f"ä¸‹è½½æˆ–æ ¡éªŒè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯ ({full_url}): {e}")
            logger.exception(f"Uncaught exception during download_file ({full_url})", exc_info=True)
            # å¦‚æœåœ¨ä¸‹è½½åã€æ ¡éªŒå‰å‘ç”Ÿé”™è¯¯ï¼Œç¡®ä¿æ¸…ç†å·²ä¸‹è½½çš„æ–‡ä»¶
            if os.path.exists(download_path) and not (expected_hash and actual_hash.lower() == expected_hash.lower()): # Check if it was a good file
                try:
                    os.remove(download_path)
                    log_info(f"å·²æ¸…ç†éƒ¨åˆ†ä¸‹è½½çš„æ–‡ä»¶ (ç”±äºåç»­é”™è¯¯): {download_path}")
                except OSError:
                    pass #å°½åŠ›è€Œä¸º
    log_error(f"æ‰€æœ‰æºå‡ä¸‹è½½å¤±è´¥æˆ–æ ¡éªŒå¤±è´¥: {url_path}") # This is logged if loop completes without returning True
    return False

def update_files(zip_path, target_dir):
    """è§£å‹å¹¶æ›¿æ¢æ–‡ä»¶ï¼Œå¸¦è¿›åº¦æ¡ï¼Œå‡å°‘æ— å…³ç»†èŠ‚è¾“å‡º"""
    temp_extract_dir = os.path.join(target_dir, "_update_temp")
    if os.path.exists(temp_extract_dir):
        shutil.rmtree(temp_extract_dir)
    os.makedirs(temp_extract_dir, exist_ok=True)
    try:
        log_info(f"å¼€å§‹è§£å‹ {zip_path} åˆ° {temp_extract_dir}...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            members = zip_ref.namelist()
            first_member = members[0]
            root_folder_in_zip = os.path.normpath(first_member).split(os.sep)[0]
            log_info(f"Zip åŒ…å†…çš„æ ¹ç›®å½•: {root_folder_in_zip}")
            if HAS_TQDM:
                for member in tqdm(members, desc='è§£å‹è¿›åº¦', ncols=70):
                    zip_ref.extract(member, temp_extract_dir)
            else:
                total = len(members)
                for idx, member in enumerate(members):
                    zip_ref.extract(member, temp_extract_dir)
                    sys.stdout.write(f"\rè§£å‹è¿›åº¦: {int((idx+1)*100/total)}%")
                    sys.stdout.flush()
                sys.stdout.write("\n")
                sys.stdout.flush()
        log_success("è§£å‹å®Œæˆã€‚")
        source_dir_to_copy = os.path.join(temp_extract_dir, root_folder_in_zip)
        if not os.path.isdir(source_dir_to_copy):
            log_warning(f"æœªåœ¨ {temp_extract_dir} ä¸­æ‰¾åˆ°é¢„æœŸçš„æ ¹ç›®å½• {root_folder_in_zip}ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨è§£å‹ç›®å½•ã€‚")
            source_dir_to_copy = temp_extract_dir
        log_info(f"å¼€å§‹æ›´æ–°æ–‡ä»¶åˆ° {target_dir}...")
        items = [item for item in os.listdir(source_dir_to_copy)]
        if HAS_TQDM:
            bar = tqdm(items, desc='æ›¿æ¢è¿›åº¦', ncols=70)
        else:
            bar = items
        for idx, item in enumerate(bar if HAS_TQDM else items):
            s_item = os.path.join(source_dir_to_copy, item)
            d_item = os.path.join(target_dir, item)
            if item == CONFIG_DIR or item in [os.path.basename(zip_path), os.path.basename(VERSION_FILE), "updater.py", "_update_temp", ".git", ".github"]:
                continue
            if os.path.isdir(s_item):
                if os.path.exists(d_item):
                    shutil.rmtree(d_item)
                shutil.copytree(s_item, d_item, dirs_exist_ok=True)
            else:
                shutil.copy2(s_item, d_item)
            if not HAS_TQDM:
                sys.stdout.write(f"\ræ›¿æ¢è¿›åº¦: {int((idx+1)*100/len(items))}%")
                sys.stdout.flush()
        if not HAS_TQDM:
            sys.stdout.write("\n")
            sys.stdout.flush()
        log_success("æ–‡ä»¶æ›´æ–°å®Œæˆã€‚")
        return True
    except zipfile.BadZipFile as e_zip:
        log_error(f"è§£å‹å¤±è´¥: {zip_path} ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„zipæ–‡ä»¶æˆ–å·²æŸå. Error: {e_zip}")
        return False
    except IOError as e_io: # More specific for file operations
        log_error(f"æ›´æ–°æ–‡ä»¶è¿‡ç¨‹ä¸­å‘ç”ŸIOé”™è¯¯: {e_io}")
        return False
    except Exception as e: # Generic fallback
        log_error(f"æ›´æ–°æ–‡ä»¶è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        logger.exception(f"Uncaught exception during update_files ({zip_path})", exc_info=True)
        return False
    finally:
        if os.path.exists(temp_extract_dir):
            shutil.rmtree(temp_extract_dir)
        if os.path.exists(zip_path):
            try:
                os.remove(zip_path)
            except OSError as e:
                log_warning(f"æ— æ³•åˆ é™¤ä¸‹è½½çš„zipæ–‡ä»¶ {zip_path}: {e}")

def write_current_version(version_str):
    """å°†å½“å‰ç‰ˆæœ¬å†™å…¥æ–‡ä»¶"""
    with open(VERSION_FILE, 'w') as f:
        f.write(str(version_str))
    logger.info(f"æœ¬åœ°ç‰ˆæœ¬å·²æ›´æ–°ä¸º: {version_str}") # Changed to logger.info

def restart_program():
    """é‡å¯å½“å‰ç¨‹åº"""
    logger.info("å‡†å¤‡é‡å¯ç¨‹åº...") # Changed to logger.info
    try:
        os.execv(sys.executable, ['python'] + sys.argv)
    except Exception as e:
        # Using log_error which internally uses logger.error
        log_error(f"âŒ é‡å¯å¤±è´¥: {e}ã€‚è¯·æ‰‹åŠ¨é‡å¯ç¨‹åºã€‚")
        logger.exception("Exception during restart_program", exc_info=True)


def check_and_update():
    """æ£€æŸ¥æ›´æ–°çš„ä¸»å‡½æ•°"""
    logger.info("æ­£åœ¨æ£€æŸ¥æ›´æ–°...") # Changed to logger.info
    current_v = get_current_version()
    logger.info(f"å½“å‰æœ¬åœ°ç‰ˆæœ¬: {current_v}") # Changed to logger.info

    latest_info = get_latest_release_info()
    if not latest_info:
        return

    latest_v_str = latest_info.get("latest_version")
    if not latest_v_str:
        log_error("APIå“åº”ä¸­æœªæ‰¾åˆ° 'latest_version' å­—æ®µã€‚æ— æ³•ç¡®å®šæœ€æ–°ç‰ˆæœ¬ã€‚")
        return
        
    latest_v = parse_version(latest_v_str)
    log_info(f"æœ€æ–°å¯ç”¨ç‰ˆæœ¬: {latest_v}")

    if latest_v > current_v:
        log_success(f"âœ¨ å‘ç°æ–°ç‰ˆæœ¬ {latest_v}ï¼")
        log_info("ğŸ“„ æ›´æ–°å†…å®¹:")
        release_notes = latest_info.get("release_notes", "æœªæä¾›æ›´æ–°æ—¥å¿—ã€‚").strip()
        for line in release_notes.split('\n'):
            logger.info(f"  {line}") # Changed to logger.info for release notes
        
        # è·å–é¢„æœŸçš„å“ˆå¸Œå€¼
        expected_sha256 = latest_info.get("sha256_hash")
        if not expected_sha256:
            log_warning("âš ï¸ APIå“åº”ä¸­æœªæä¾› 'sha256_hash' å­—æ®µã€‚")
            log_error("ä¸ºäº†å®‰å…¨èµ·è§ï¼Œç¼ºå°‘å“ˆå¸Œå€¼æ—¶å°†ä¸­æ­¢æ›´æ–°ã€‚è¯·è”ç³»å¼€å‘è€…æ›´æ–°APIä»¥åŒ…å«å“ˆå¸Œå€¼ã€‚")
            # ä¸¥æ ¼æ¨¡å¼ï¼šå¦‚æœå“ˆå¸Œç¼ºå¤±ï¼Œåˆ™ä¸­æ­¢
            # å¦‚æœå¸Œæœ›éä¸¥æ ¼æ¨¡å¼ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹ä¸€è¡Œå¹¶è°ƒæ•´ download_file çš„è°ƒç”¨
            return 
        else:
            log_info(f"é¢„æœŸæ–‡ä»¶ SHA256 å“ˆå¸Œ: {expected_sha256}")

        user_input = input(f"â“ {LogColor.YELLOW}æ˜¯å¦è¦ä¸‹è½½å¹¶æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬? (y/N): {LogColor.END}").strip().lower()
        if user_input != 'y':
            log_info("ç”¨æˆ·å–æ¶ˆæ›´æ–°ã€‚")
            return

        log_info("ğŸš€ å¼€å§‹æ›´æ–°...")
        zip_url_from_api = latest_info.get("source_code_zip_url") 
        # source_code_zip_url é€šå¸¸æ˜¯ç±»ä¼¼ https://api.github.com/repos/owner/repo/zipball/v1.0.0
        # æˆ‘ä»¬éœ€è¦çš„æ˜¯ä¸‹è½½è·¯å¾„ owner/repo/archive/refs/tags/v1.0.0.zip

        if not zip_url_from_api: # ç®€å•æ£€æŸ¥
            log_error(f"APIè¿”å›çš„ source_code_zip_url æ— æ•ˆ: {zip_url_from_api}")
            return

        # å°è¯•ä»API URLä¸­æå–ä»“åº“æ‰€æœ‰è€…å’Œåç§°åŠç‰ˆæœ¬æ ‡ç­¾æ¥æ„é€ æ›´é€šç”¨çš„ä¸‹è½½è·¯å¾„
        # å‡è®¾API URLæ ¼å¼ä¸º "https://api.github.com/repos/{owner}/{repo}/zipball/{tag}"
        # æˆ– "https://github.com/{owner}/{repo}/archive/refs/tags/{tag}.zip"
        # æˆ‘ä»¬éœ€è¦ "{owner}/{repo}/archive/refs/tags/{tag}.zip" æ ¼å¼ç»™é•œåƒ
        
        repo_path_for_mirrors = None
        # ä¼˜å…ˆä½¿ç”¨ tag æ„å»ºè·¯å¾„
        tag_name = latest_v_str # é€šå¸¸ç‰ˆæœ¬å·å°±æ˜¯tag
        
        # è§£æ zip_url_from_api ä»¥è·å– owner/repo
        try:
            parsed_api_url = urllib.parse.urlparse(zip_url_from_api)
            path_parts = parsed_api_url.path.strip('/').split('/')
            if "api.github.com" in parsed_api_url.netloc and len(path_parts) >= 4 and path_parts[0] == "repos":
                owner, repo = path_parts[1], path_parts[2]
                repo_path_for_mirrors = f"{owner}/{repo}/archive/refs/tags/{tag_name}.zip"
            # å¦‚æœAPIç›´æ¥ç»™çš„æ˜¯GitHubçš„å½’æ¡£zipé“¾æ¥
            elif "github.com" in parsed_api_url.netloc and path_parts[-1].endswith(".zip") and "archive" in parsed_api_url.path:
                 # å°è¯•æå– owner/repo/archive/refs/tags/tag.zip æ ¼å¼
                # e.g. /user/repo/archive/refs/tags/v1.0.0.zip
                if len(path_parts) >= 5 and path_parts[-3] == "tags" and path_parts[-4] == "refs" and path_parts[-5] == "archive":
                    repo_path_for_mirrors = "/".join(path_parts[-6:]) # owner/repo/archive/refs/tags/tag.zip
                else: # Fallback to a simpler extraction if structure is different
                    repo_path_for_mirrors = "/".join(path_parts[0:2]) + f"/archive/refs/tags/{tag_name}.zip"

            if not repo_path_for_mirrors:
                 raise ValueError("Could not determine repository path from API URL")

        except Exception as e:
            log_error(f"æ— æ³•ä»API URL '{zip_url_from_api}' è§£æä»“åº“ä¿¡æ¯: {e}")
            log_warning("å°†å°è¯•ä½¿ç”¨åŸºäºç‰ˆæœ¬å·çš„é€šç”¨è·¯å¾„ã€‚")
            # å¦‚æœæ— æ³•è§£æï¼Œå¯ä»¥å°è¯•ä¸€ä¸ªåŸºäºå·²çŸ¥ç»“æ„çš„çŒœæµ‹ï¼Œä½†è¿™ä¸å¤ªå¯é 
            # æ­¤å¤„åº”æœ‰æ›´å¥å£®çš„ owner/repo è·å–æ–¹å¼ï¼Œæˆ–è€…APIç›´æ¥æä¾›è§„èŒƒçš„ä¸‹è½½è·¯å¾„
            # For now, we might have to rely on a pre-configured repo slug if parsing fails.
            # For this exercise, we'll assume a generic path might be formed if parsing fails,
            # or simply error out if a reliable path can't be formed.
            # Let's assume 'latest_info' might contain 'repo_slug' (e.g., "owner/repo")
            repo_slug_from_api = latest_info.get("repo_slug") # e.g. "username/repository"
            if repo_slug_from_api:
                repo_path_for_mirrors = f"{repo_slug_from_api}/archive/refs/tags/{tag_name}.zip"
            else:
                log_error("API æœªæä¾› repo_slug ä¸”æ— æ³•ä» source_code_zip_url è§£æã€‚æ— æ³•æ„é€ ä¸‹è½½è·¯å¾„ã€‚")
                return

        download_target_zip = "_latest_version.zip"
        
        log_info(f"å°†å°è¯•ä»é•œåƒæºä¸‹è½½: {repo_path_for_mirrors}")

        # è°ƒç”¨ download_file å¹¶ä¼ å…¥é¢„æœŸçš„å“ˆå¸Œå€¼
        if download_file(repo_path_for_mirrors, download_target_zip, expected_sha256):
            if update_files(download_target_zip, "."): # update_files ä¼šåœ¨ finally ä¸­åˆ é™¤ zip
                write_current_version(latest_v_str)
                log_success("ğŸ‰ æ›´æ–°æˆåŠŸï¼ç¨‹åºå³å°†é‡å¯ã€‚")
                restart_program()
            else:
                log_error("âŒ æ›´æ–°å¤±è´¥ï¼Œæ–‡ä»¶æ›¿æ¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ã€‚è¯·æ£€æŸ¥æ—¥å¿—ã€‚")
                # download_file æˆåŠŸåï¼Œzipæ–‡ä»¶å¯èƒ½å·²è¢« update_files åˆ é™¤æˆ–ä¿ç•™
                # å¦‚æœ update_files å¤±è´¥ï¼Œå®ƒä¼šå°è¯•åˆ é™¤ zipã€‚å¦‚æœå“ˆå¸Œæ ¡éªŒå¤±è´¥ï¼Œdownload_file ä¼šåˆ é™¤ã€‚
        else:
            # download_file è¿”å› False æ„å‘³ç€ä¸‹è½½å¤±è´¥æˆ–å“ˆå¸Œæ ¡éªŒå¤±è´¥
            # é”™è¯¯ä¿¡æ¯å·²åœ¨ download_file å†…éƒ¨æ‰“å°
            log_error("âŒ æ›´æ–°å¤±è´¥ã€‚ä¸‹è½½æˆ–æ–‡ä»¶æ ¡éªŒæœªé€šè¿‡ã€‚")
            # download_file åº”è¯¥å·²ç»åˆ é™¤äº†æ–‡ä»¶å¦‚æœå“ˆå¸Œä¸åŒ¹é…
            if os.path.exists(download_target_zip):
                 try:
                     os.remove(download_target_zip) # å†æ¬¡å°è¯•ç¡®ä¿åˆ é™¤
                     log_info(f"å·²æ¸…ç†ä¸‹è½½æ–‡ä»¶: {download_target_zip}")
                 except OSError as e:
                     log_warning(f"âš ï¸ æ¸…ç†ä¸‹è½½æ–‡ä»¶ {download_target_zip} å¤±è´¥: {e}")
    else:
        log_success("âœ… å½“å‰å·²æ˜¯æœ€æ–°ç‰ˆæœ¬ã€‚")

if __name__ == "__main__":
    from logger import setup_logging # Import setup_logging for the test execution
    # Setup basic logging for the __main__ execution
    # In a real app, this would be in the main entry point.
    setup_logging(level="DEBUG") 
    
    # For testing, ensure urllib.parse is available for the modified check_and_update
    # import urllib.parse # This is already imported at the top level
    
    logger.info("Updater script started directly.")
    check_and_update()
    logger.info("Updater script finished.")